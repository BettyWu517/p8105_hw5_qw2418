---
title: "Homework5"
author: "Qianying Wu"
date: "2023-11-12"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
```

## Problem 1
For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |>
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) |> 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df |>
  select(city_state, disposition, resolution) |> 
  group_by(city_state) |>
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") |> pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") |> pull(hom_total)) 

broom::tidy(bmore_test) |>
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df |>
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) |>
  select(-prop_tests) |>
  unnest(tidy_tests) |>
  select(city_state, estimate, conf.low, conf.high) |>
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides. 

## Problem 2

```{r}

# List all CSV files in the directory
file_paths <- list.files(path = "data/problem2/", pattern = "\\.csv$", full.names = TRUE)

library(tidyverse)

process_file <- function(file_path) {
  # Read the file
  data <- read.csv(file_path)

  subject_id <- strsplit(basename(file_path), "[_.]")[[1]][2]

  if (grepl("con", file_path)) {
    arm <- "Control"
    }
  else{
    arm <- "Experimental"
    }

  data <- mutate(data, `Subject ID` = subject_id, Arm = arm)
  return(data)
}


combined_data <- purrr::map_df(file_paths, process_file)

combined_data


combined_data <- combined_data |> pivot_longer(
    cols = starts_with("week_"), 
    names_to = "Week",           
    values_to = "Observation"   
  ) |>
  mutate(
    Week = parse_number(Week),
    Arm = factor(Arm, levels = c("Control", "Experimental"))
  )

head(combined_data)

## Draw the spaghetti plot
ggplot(combined_data, aes(x = Week, y = Observation, group = `Subject ID`, color = Arm)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ Arm, nrow = 1) + 
  labs(title = "Spaghetti Plot of Subject Observations in different Weeks",
       x = "Week",
       y = "Observation",
       color = "Group") +
  scale_color_manual(values = c("Control" = "blue", "Experimental" = "red"))

ggsave("spaghetti plot.jpg")

```

*comment on differences between groups *:
In the control group, when the observations are relatively stable across weeks. However, in the experimental group, the observation increases when week increases. 


## Problem 3



First set the following design elements:

Fix n=30
Fix σ=5
Set μ=0. Generate 5000 datasets from the model

x∼Normal[μ,σ]

For each dataset, save μ̂ and the p-value arising from a test of H:μ=0 using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

```{r}
n <- 30
sigma <- 5
mu <- c(0, 1, 2, 3, 4, 5, 6)
iteration <- 5000
alpha <- 0.05



stat_test <- function(n, sigma, mu, iteration, alpha){
  pval_list <- c()
  mu_hat_list <- c()
  rej_est_list <- c()
  
  for (i in 1:iteration){
    sample <- rnorm(n, mu, sigma)
    clean <- broom::tidy(t.test(sample, mu = 0))
    pvalue <- clean$p.value
    pval_list <- c(pval_list, pvalue)
    mu_hat <- clean$estimate
    mu_hat_list <- c(mu_hat_list, mu_hat)
    
    # store the rej_est_list with all the estimates that are rejected
    if (pvalue < alpha ){
      rej_est <- clean$estimate
      rej_est_list <- c(rej_est_list, rej_est)
    }
    
  }
  power <- mean(pval_list < alpha)
  mean_estimate <- mean(mu_hat_list)
  mean_est_rej <- mean(rej_est_list)


  
  return(c(power, mean_estimate, mean_est_rej))
}


power <- c()
mean_estimate <- c()
mean_est_rej <- c()

for (i in mu){
  result <- stat_test(n, sigma, i, iteration, alpha)
  power <- c(power, result[1])
  mean_estimate <- c(mean_estimate, result[2])
  mean_est_rej <- c(mean_est_rej, result[3])
  
}

power
mean_estimate
mean_est_rej

```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.

```{r}
df <- data.frame(mu = mu, power = power, mean_estimate = mean_estimate, mean_est_rej = mean_est_rej)
df

ggplot(df, aes(x = mu, y = power)) +
  geom_point() +
  geom_line() +
  labs(title = "Power vs True Value of μ", x = "True Value of μ", y = "Power")

ggsave("Power vs True Value of mu.jpg")

```


Make a plot showing the average estimate of μ̂ on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of μ̂ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 

```{r}
ggplot(df) +
  geom_point(aes(x = mu, y = mean_estimate, color = 'Average μ̂')) +
  geom_line(aes(x = mu, y = mean_estimate, color = 'Average μ̂')) +
  geom_point(aes(x = mu, y = mean_est_rej, color = 'Average μ̂ (Null Rejected)')) +
  geom_line(aes(x = mu, y = mean_est_rej, color = 'Average μ̂ (Null Rejected)'), linetype = "dashed") +
  labs(title = "Average Estimate of μ̂ vs True Value of mu", 
       x = "True Value of μ", 
       y = "Average Estimate of μ̂",
       color = "Legend") +
  scale_color_manual(values = c('Average μ̂' = 'blue', 'Average μ̂ (Null Rejected)' = 'red'))

ggsave("average estimuate of mu_bar and the true value of mu.jpg")

```

Is the sample average of μ̂ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

The sample average of  μ̂ across tests for which the null is rejected is only approximately equal to the true value of μ when the true value of μ is 3 or larger. This is because the power of the test is lower for smaller effect sizes, leading to a selection bias in which only the more extreme sample means lead to rejection of the null.

